{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XxIk0jMdsDvM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FF8HYqnr-7d"
      },
      "outputs": [],
      "source": [
        "Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\\#1 What is Simple Linear Regression\n",
        "\n",
        "Simple Linear Regression is a statistical method used to model the relationship between two continuous variables — one independent (X) and one dependent (Y). It fits a straight line (Y = mX + c) to predict Y based on X.\n",
        "\n",
        "---\n",
        "\n",
        "\\#2 What are the key assumptions of Simple Linear Regression\n",
        "\n",
        "The assumptions include linearity between variables, independence of errors, homoscedasticity (constant variance of errors), and normally distributed residuals. These ensure the reliability of predictions and coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "\\#3 What does the coefficient m represent in the equation Y = mX + c\n",
        "\n",
        "The coefficient **m** represents the slope of the regression line. It indicates the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n",
        "\n",
        "---\n",
        "\n",
        "\\#4 What does the intercept c represent in the equation Y = mX + c\n",
        "\n",
        "The intercept **c** is the value of Y when X equals zero. It represents the starting point of the regression line on the Y-axis.\n",
        "\n",
        "---\n",
        "\n",
        "\\#5 How do we calculate the slope m in Simple Linear Regression\n",
        "\n",
        "The slope is calculated using the formula:\n",
        "**m = Σ((X - X̄)(Y - Ȳ)) / Σ((X - X̄)²)**.\n",
        "It measures how much Y changes with a unit change in X.\n",
        "\n",
        "---\n",
        "\n",
        "\\#6 What is the purpose of the least squares method in Simple Linear Regression\n",
        "\n",
        "The least squares method minimizes the sum of the squared differences between the actual and predicted Y values. It helps find the best-fitting line through the data points.\n",
        "\n",
        "---\n",
        "\n",
        "\\#7 How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "\n",
        "R² indicates how much of the variance in the dependent variable is explained by the independent variable. An R² of 0.8 means 80% of the variation in Y is explained by X.\n",
        "\n",
        "---\n",
        "\n",
        "\\#8 What is Multiple Linear Regression\n",
        "\n",
        "Multiple Linear Regression involves predicting a dependent variable using two or more independent variables. The model has the form Y = b0 + b1X1 + b2X2 + ... + bnXn.\n",
        "\n",
        "---\n",
        "\n",
        "\\#9 What is the main difference between Simple and Multiple Linear Regression\n",
        "\n",
        "Simple Linear Regression uses one independent variable, whereas Multiple Linear Regression uses two or more. This allows MLR to capture more complex relationships.\n",
        "\n",
        "---\n",
        "\n",
        "\\#10 What are the key assumptions of Multiple Linear Regression\n",
        "\n",
        "The assumptions include linearity, independence, no multicollinearity, homoscedasticity, and normally distributed residuals. Violating these can affect model reliability.\n",
        "\n",
        "---\n",
        "\n",
        "\\#11 What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "\n",
        "Heteroscedasticity means the variance of residuals changes across levels of an independent variable. It can lead to inefficient estimates and invalid significance tests.\n",
        "\n",
        "---\n",
        "\n",
        "\\#12 How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "\n",
        "Multicollinearity can be reduced by removing highly correlated predictors, using Principal Component Analysis (PCA), or applying regularization techniques like Ridge or Lasso regression.\n",
        "\n",
        "---\n",
        "\n",
        "\\#13 What are some common techniques for transforming categorical variables for use in regression models\n",
        "\n",
        "Common techniques include one-hot encoding, label encoding, and using dummy variables. These methods convert categories into numerical values suitable for regression.\n",
        "\n",
        "---\n",
        "\n",
        "\\#14 What is the role of interaction terms in Multiple Linear Regression\n",
        "\n",
        "Interaction terms allow the effect of one independent variable on the dependent variable to depend on another variable. This helps model more complex relationships.\n",
        "\n",
        "---\n",
        "\n",
        "\\#15 How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "\n",
        "In Simple Linear Regression, the intercept represents Y when X = 0. In MLR, it represents the expected Y when all independent variables are zero — which may not always be meaningful.\n",
        "\n",
        "---\n",
        "\n",
        "\\#16 What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "\n",
        "The slope shows how much the dependent variable is expected to increase or decrease with a one-unit increase in an independent variable, holding others constant.\n",
        "\n",
        "---\n",
        "\n",
        "\\#17 How does the intercept in a regression model provide context for the relationship between variables\n",
        "\n",
        "The intercept gives a baseline value for the dependent variable when all predictors are zero. It helps understand the starting point or reference level in a model.\n",
        "\n",
        "---\n",
        "\n",
        "\\#18 What are the limitations of using R² as a sole measure of model performance\n",
        "\n",
        "R² doesn't account for overfitting, doesn't indicate whether a regression model is appropriate, and can increase with more predictors regardless of their relevance.\n",
        "\n",
        "---\n",
        "\n",
        "\\#19 How would you interpret a large standard error for a regression coefficient\n",
        "\n",
        "A large standard error suggests the coefficient estimate is not precise, possibly due to high variability or multicollinearity. It may indicate the predictor isn't statistically significant.\n",
        "\n",
        "---\n",
        "\n",
        "\\#20 How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "\n",
        "Heteroscedasticity appears as a funnel shape in residual plots. It violates regression assumptions, leading to unreliable confidence intervals and significance tests.\n",
        "\n",
        "---\n",
        "\n",
        "\\#21 What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        "\n",
        "It suggests the model includes unnecessary predictors that don't improve explanatory power. Adjusted R² penalizes the addition of irrelevant variables.\n",
        "\n",
        "---\n",
        "\n",
        "\\#22 Why is it important to scale variables in Multiple Linear Regression\n",
        "\n",
        "Scaling ensures all features contribute equally to the model, especially important when using regularization. It also speeds up convergence and improves interpretability.\n",
        "\n",
        "---\n",
        "\n",
        "\\#23 What is polynomial regression\n",
        "\n",
        "Polynomial regression is an extension of linear regression where the relationship between X and Y is modeled as an nth-degree polynomial (e.g., Y = b0 + b1X + b2X² + ...).\n",
        "\n",
        "---\n",
        "\n",
        "\\#24 How does polynomial regression differ from linear regression\n",
        "\n",
        "Polynomial regression fits curves instead of straight lines by including higher-degree terms of the predictor variable. It's used when the relationship is non-linear.\n",
        "\n",
        "---\n",
        "\n",
        "\\#25 When is polynomial regression used\n",
        "\n",
        "It’s used when data shows a curvilinear relationship. For example, growth curves or trends that accelerate or decelerate over time.\n",
        "\n",
        "---\n",
        "\n",
        "\\#26 What is the general equation for polynomial regression\n",
        "\n",
        "The general form is:\n",
        "**Y = b0 + b1X + b2X² + b3X³ + ... + bnXⁿ**\n",
        "where n is the degree of the polynomial.\n",
        "\n",
        "---\n",
        "\n",
        "\\#27 Can polynomial regression be applied to multiple variables\n",
        "\n",
        "Yes, polynomial regression can be extended to multiple variables by including interaction and higher-degree terms for each variable, resulting in complex models.\n",
        "\n",
        "---\n",
        "\n",
        "\\#28 What are the limitations of polynomial regression\n",
        "\n",
        "It may overfit the data, especially with high degrees. It also becomes complex and harder to interpret. Extrapolation outside the data range can be unreliable.\n",
        "\n",
        "---\n",
        "\n",
        "\\#29 What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "\n",
        "Use metrics like R², adjusted R², cross-validation, and residual plots. Regularization techniques can help prevent overfitting when the degree increases.\n",
        "\n",
        "---\n",
        "\n",
        "\\#30 Why is visualization important in polynomial regression\n",
        "\n",
        "Visualization helps understand the shape of the fitted curve, detect overfitting, and interpret the model's behavior across the data range.\n",
        "\n",
        "---\n",
        "\n",
        "\\#31 How is polynomial regression implemented in Python\n",
        "\n",
        "Polynomial regression is implemented using libraries like scikit-learn with `PolynomialFeatures` to transform inputs and `LinearRegression` to fit the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "nWlsm7C3sHL7"
      }
    }
  ]
}